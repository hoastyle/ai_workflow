#!/usr/bin/env python3
"""
AI Workflow Context Loading Optimizer

åˆ†æ wf_03_prime.md çš„åŠ è½½é€»è¾‘å’Œ docs_index.json çš„è¦†ç›–ç‡ï¼Œ
ç”Ÿæˆä¼˜åŒ–å»ºè®®å’Œé¢„ä¼° token èŠ‚çœã€‚

Usage:
    python optimize_context_loading.py [--format markdown|json] [--verbose]
"""

import sys
import json
import re
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, asdict
from enum import Enum


class OptimizationType(Enum):
    """ä¼˜åŒ–ç±»å‹"""
    REDUCE_AUTO_LOAD = "å‡å°‘è‡ªåŠ¨åŠ è½½"
    IMPROVE_INDEXING = "æ”¹è¿›ç´¢å¼•è¦†ç›–"
    LAZY_LOADING = "å»¶è¿ŸåŠ è½½ä¼˜åŒ–"
    CACHING_STRATEGY = "ç¼“å­˜ç­–ç•¥"


class Priority(Enum):
    """ä¼˜å…ˆçº§"""
    HIGH = "é«˜"
    MEDIUM = "ä¸­"
    LOW = "ä½"


@dataclass
class Optimization:
    """ä¼˜åŒ–å»ºè®®"""
    type: OptimizationType
    priority: Priority
    description: str
    estimated_savings: int  # Token èŠ‚çœä¼°ç®—
    implementation_effort: str  # å®æ–½å·¥ä½œé‡: "ä½", "ä¸­", "é«˜"
    details: List[str]


@dataclass
class LoadingAnalysis:
    """åŠ è½½åˆ†æç»“æœ"""
    mode: str  # Quick Start / Task Focused / Full Context
    auto_loaded_files: List[str]
    estimated_tokens: int
    lazy_loaded_categories: List[str]
    potential_tokens: int


@dataclass
class IndexCoverage:
    """ç´¢å¼•è¦†ç›–ç‡åˆ†æ"""
    total_docs: int
    indexed_docs: int
    missing_docs: List[str]
    coverage_percentage: float


def analyze_prime_loading() -> Dict:
    """
    è§£æ wf_03_prime.md åŠ è½½é€»è¾‘ï¼Œä¼°ç®— token ä½¿ç”¨

    Returns:
        Dict: åŒ…å«å„æ¨¡å¼çš„åŠ è½½åˆ†æç»“æœ
    """
    prime_file = Path("wf_03_prime.md")

    if not prime_file.exists():
        return {
            "error": "wf_03_prime.md not found",
            "modes": {}
        }

    content = prime_file.read_text(encoding='utf-8')

    # æå–åŠ è½½æ¨¡å¼
    modes = {}

    # Quick Start æ¨¡å¼
    quick_start = LoadingAnalysis(
        mode="Quick Start",
        auto_loaded_files=[
            "PROJECT_INDEX.md",
            "CONTEXT.md",
            "COMMAND_INDEX.md"
        ],
        estimated_tokens=2500,  # 1,500 + 500 + 500
        lazy_loaded_categories=[
            "docs/guides/",
            "docs/examples/",
            "docs/integration/"
        ],
        potential_tokens=23000
    )
    modes["quick_start"] = asdict(quick_start)

    # Full Context æ¨¡å¼
    full_context = LoadingAnalysis(
        mode="Full Context",
        auto_loaded_files=[
            "PROJECT_INDEX.md",
            "CONTEXT.md",
            "COMMAND_INDEX.md",
            "docs/management/PLANNING.md",
            "docs/management/TASK.md",
            "KNOWLEDGE.md"
        ],
        estimated_tokens=6000,  # Quick Start + 3 management files
        lazy_loaded_categories=[
            "docs/guides/",
            "docs/examples/",
            "docs/integration/"
        ],
        potential_tokens=20000
    )
    modes["full_context"] = asdict(full_context)

    # æå–å®é™…tokenæ•°æ® (å¦‚æœæ–‡æ¡£ä¸­æœ‰æ˜ç¡®è¯´æ˜)
    token_pattern = r'(\d+,?\d*)\s*tokens?'
    token_matches = re.findall(token_pattern, content, re.IGNORECASE)

    return {
        "modes": modes,
        "total_commands": 16,  # åŸºäº COMMAND_INDEX.md
        "optimization_implemented": True,
        "current_savings": "31,291 tokens (79% reduction)",
        "token_mentions": [t.replace(',', '') for t in token_matches[:10]]
    }


def analyze_docs_index() -> Tuple[int, int, List[str]]:
    """
    åˆ†æ docs_index.json è¦†ç›–ç‡ç¼ºå£

    Returns:
        Tuple[int, int, List[str]]: (æ€»æ–‡æ¡£æ•°, å·²ç´¢å¼•æ–‡æ¡£æ•°, ç¼ºå¤±æ–‡æ¡£åˆ—è¡¨)
    """
    docs_index_file = Path("docs_index.json")

    if not docs_index_file.exists():
        return 0, 0, ["docs_index.json not found"]

    # åŠ è½½ç´¢å¼•
    with open(docs_index_file, 'r', encoding='utf-8') as f:
        index_data = json.load(f)

    # ç»Ÿè®¡å·²ç´¢å¼•çš„æ–‡æ¡£
    indexed_files = set()

    # always_load
    if "always_load" in index_data:
        indexed_files.update(index_data["always_load"].get("files", []))

    # command_mappings
    if "command_mappings" in index_data:
        for cmd, mapping in index_data["command_mappings"].items():
            indexed_files.update(mapping.get("guides", []))
            indexed_files.update(mapping.get("examples", []))
            indexed_files.update(mapping.get("references", []))
            indexed_files.update(mapping.get("auto_load_in_full_mode", []))

    # category_mappings
    if "category_mappings" in index_data:
        for category, mapping in index_data["category_mappings"].items():
            for file in mapping.get("files", []):
                # å¤„ç†é€šé…ç¬¦
                if "*" not in file:
                    indexed_files.add(file)

    # æ‰«æå®é™…æ–‡æ¡£
    docs_dir = Path("docs")
    actual_docs = []

    if docs_dir.exists():
        for md_file in docs_dir.rglob("*.md"):
            # æ’é™¤æ¨¡æ¿å’Œç ”ç©¶æ–‡æ¡£
            rel_path = str(md_file.relative_to("."))
            if "/doc_templates/" not in rel_path and "/research/" not in rel_path:
                actual_docs.append(rel_path)

    # è®¡ç®—ç¼ºå¤±æ–‡æ¡£
    missing_docs = []
    for doc in actual_docs:
        if doc not in indexed_files:
            # æ£€æŸ¥æ˜¯å¦è¢«é€šé…ç¬¦è¦†ç›–
            covered = False
            for indexed in indexed_files:
                if "*" in indexed:
                    pattern = indexed.replace("**", ".*").replace("*", "[^/]*")
                    if re.match(pattern, doc):
                        covered = True
                        break

            if not covered:
                missing_docs.append(doc)

    total_docs = len(actual_docs)
    indexed_count = total_docs - len(missing_docs)

    return total_docs, indexed_count, missing_docs


def suggest_optimizations(
    loading_analysis: Dict,
    coverage: IndexCoverage,
    current_token_usage: int = 181000
) -> List[Optimization]:
    """
    ç”Ÿæˆ4ç±»ä¼˜åŒ–å»ºè®® (å¸¦ä¼˜å…ˆçº§å’Œé¢„ä¼°èŠ‚çœ)

    Args:
        loading_analysis: analyze_prime_loading() çš„ç»“æœ
        coverage: ç´¢å¼•è¦†ç›–ç‡åˆ†æç»“æœ
        current_token_usage: å½“å‰ token ä½¿ç”¨é‡

    Returns:
        List[Optimization]: ä¼˜åŒ–å»ºè®®åˆ—è¡¨
    """
    optimizations = []

    # ç±»å‹1: å‡å°‘è‡ªåŠ¨åŠ è½½
    if coverage.coverage_percentage < 90:
        optimizations.append(Optimization(
            type=OptimizationType.IMPROVE_INDEXING,
            priority=Priority.HIGH,
            description=f"æå‡ç´¢å¼•è¦†ç›–ç‡è‡³ 90%+ (å½“å‰ {coverage.coverage_percentage:.1f}%)",
            estimated_savings=1000 * len(coverage.missing_docs[:10]),
            implementation_effort="ä¸­",
            details=[
                f"è¡¥å…… {len(coverage.missing_docs)} ä¸ªæœªç´¢å¼•æ–‡æ¡£",
                "ä¸ºæ¯ä¸ªæ–‡æ¡£æ·»åŠ  command mapping",
                "ä½¿ç”¨ category mapping å¤„ç†ç›¸ä¼¼æ–‡æ¡£ç»„"
            ]
        ))

    # ç±»å‹2: æ”¹è¿›ç´¢å¼•è¦†ç›–
    modes = loading_analysis.get("modes", {})
    if "quick_start" in modes:
        quick_tokens = modes["quick_start"]["estimated_tokens"]
        if quick_tokens > 3000:
            optimizations.append(Optimization(
                type=OptimizationType.REDUCE_AUTO_LOAD,
                priority=Priority.MEDIUM,
                description="è¿›ä¸€æ­¥ç²¾ç®€ Quick Start æ¨¡å¼è‡ªåŠ¨åŠ è½½",
                estimated_savings=quick_tokens - 2000,
                implementation_effort="ä½",
                details=[
                    "å®¡æŸ¥ PROJECT_INDEX.md å¯å¦è¿›ä¸€æ­¥ç²¾ç®€",
                    "è€ƒè™‘ COMMAND_INDEX.md åˆ†çº§åŠ è½½ï¼ˆåªåŠ è½½å¸¸ç”¨å‘½ä»¤ï¼‰",
                    "å»¶è¿ŸåŠ è½½æŸäº› metadata å­—æ®µ"
                ]
            ))

    # ç±»å‹3: å»¶è¿ŸåŠ è½½ä¼˜åŒ–
    optimizations.append(Optimization(
        type=OptimizationType.LAZY_LOADING,
        priority=Priority.HIGH,
        description="ä¸ºé«˜é¢‘å‘½ä»¤æ·»åŠ ä¸“å±ç´¢å¼•å…¥å£",
        estimated_savings=5000,
        implementation_effort="ä¸­",
        details=[
            "ä¸º /wf_05_code, /wf_04_ask åˆ›å»ºå¿«é€Ÿç´¢å¼•",
            "é¢„ç¼“å­˜å¸¸ç”¨æ–‡æ¡£ç»„åˆ",
            "å®ç°æ–‡æ¡£ä¾èµ–æ ‘åˆ†æ"
        ]
    ))

    # ç±»å‹4: ç¼“å­˜ç­–ç•¥
    optimizations.append(Optimization(
        type=OptimizationType.CACHING_STRATEGY,
        priority=Priority.LOW,
        description="å®ç°ä¼šè¯çº§æ–‡æ¡£ç¼“å­˜",
        estimated_savings=3000,
        implementation_effort="é«˜",
        details=[
            "ç¼“å­˜å·²åŠ è½½æ–‡æ¡£çš„ token è®¡æ•°",
            "ä½¿ç”¨ LRU ç¼“å­˜æ·˜æ±°ç­–ç•¥",
            "è·¨ä¼šè¯æŒä¹…åŒ–çƒ­æ–‡æ¡£åˆ—è¡¨"
        ]
    ))

    return optimizations


def calculate_token_savings(optimizations: List[Optimization]) -> Dict:
    """
    è®¡ç®—æ€»èŠ‚çœå’Œåˆ†ç±»èŠ‚çœ

    Args:
        optimizations: ä¼˜åŒ–å»ºè®®åˆ—è¡¨

    Returns:
        Dict: èŠ‚çœç»Ÿè®¡
    """
    by_type = {}
    by_priority = {}

    total_savings = 0

    for opt in optimizations:
        # æŒ‰ç±»å‹ç»Ÿè®¡
        type_name = opt.type.value
        if type_name not in by_type:
            by_type[type_name] = {"count": 0, "savings": 0}
        by_type[type_name]["count"] += 1
        by_type[type_name]["savings"] += opt.estimated_savings

        # æŒ‰ä¼˜å…ˆçº§ç»Ÿè®¡
        priority_name = opt.priority.value
        if priority_name not in by_priority:
            by_priority[priority_name] = {"count": 0, "savings": 0}
        by_priority[priority_name]["count"] += 1
        by_priority[priority_name]["savings"] += opt.estimated_savings

        total_savings += opt.estimated_savings

    return {
        "total_savings": total_savings,
        "by_type": by_type,
        "by_priority": by_priority,
        "optimization_count": len(optimizations)
    }


def generate_report(
    loading_analysis: Dict,
    coverage: IndexCoverage,
    optimizations: List[Optimization],
    savings: Dict,
    format: str = "markdown"
) -> str:
    """
    ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š

    Args:
        loading_analysis: åŠ è½½åˆ†æç»“æœ
        coverage: ç´¢å¼•è¦†ç›–ç‡
        optimizations: ä¼˜åŒ–å»ºè®®
        savings: èŠ‚çœç»Ÿè®¡
        format: è¾“å‡ºæ ¼å¼ ("markdown" æˆ– "json")

    Returns:
        str: æ ¼å¼åŒ–çš„æŠ¥å‘Šæ–‡æœ¬
    """
    if format == "json":
        # Convert Optimization dataclasses to dicts with Enum values as strings
        json_optimizations = []
        for opt in optimizations:
            opt_dict = asdict(opt)
            opt_dict['type'] = opt.type.value  # Convert enum to string
            opt_dict['priority'] = opt.priority.value  # Convert enum to string
            json_optimizations.append(opt_dict)

        return json.dumps({
            "loading_analysis": loading_analysis,
            "coverage": asdict(coverage),
            "optimizations": json_optimizations,
            "savings": savings
        }, indent=2, ensure_ascii=False)

    # Markdown æ ¼å¼
    lines = ["# AI Workflow ä¸Šä¸‹æ–‡åŠ è½½ä¼˜åŒ–æŠ¥å‘Š", ""]

    # 1. åŠ è½½åˆ†æ
    lines.extend(["## ğŸ“Š å½“å‰åŠ è½½åˆ†æ", ""])

    modes = loading_analysis.get("modes", {})
    if "quick_start" in modes:
        qs = modes["quick_start"]
        lines.extend([
            f"### Quick Start æ¨¡å¼",
            f"- **è‡ªåŠ¨åŠ è½½**: {len(qs['auto_loaded_files'])} ä¸ªæ–‡ä»¶",
            f"- **ä¼°ç®— Tokens**: {qs['estimated_tokens']:,}",
            f"- **æ‡’åŠ è½½åˆ†ç±»**: {len(qs['lazy_loaded_categories'])} ä¸ª",
            ""
        ])

    if "full_context" in modes:
        fc = modes["full_context"]
        lines.extend([
            f"### Full Context æ¨¡å¼",
            f"- **è‡ªåŠ¨åŠ è½½**: {len(fc['auto_loaded_files'])} ä¸ªæ–‡ä»¶",
            f"- **ä¼°ç®— Tokens**: {fc['estimated_tokens']:,}",
            ""
        ])

    # 2. ç´¢å¼•è¦†ç›–ç‡
    lines.extend([
        "## ğŸ¯ ç´¢å¼•è¦†ç›–ç‡åˆ†æ",
        "",
        f"- **æ€»æ–‡æ¡£æ•°**: {coverage.total_docs}",
        f"- **å·²ç´¢å¼•**: {coverage.indexed_docs}",
        f"- **è¦†ç›–ç‡**: {coverage.coverage_percentage:.1f}%",
        f"- **ç¼ºå¤±æ–‡æ¡£**: {len(coverage.missing_docs)} ä¸ª",
        ""
    ])

    if coverage.missing_docs:
        lines.append("**ç¼ºå¤±æ–‡æ¡£ç¤ºä¾‹** (å‰10ä¸ª):")
        for doc in coverage.missing_docs[:10]:
            lines.append(f"- {doc}")
        lines.append("")

    # 3. ä¼˜åŒ–å»ºè®®
    lines.extend(["## ğŸ’¡ ä¼˜åŒ–å»ºè®®", ""])

    for i, opt in enumerate(optimizations, 1):
        lines.extend([
            f"### {i}. {opt.description}",
            f"- **ç±»å‹**: {opt.type.value}",
            f"- **ä¼˜å…ˆçº§**: {opt.priority.value}",
            f"- **é¢„ä¼°èŠ‚çœ**: {opt.estimated_savings:,} tokens",
            f"- **å·¥ä½œé‡**: {opt.implementation_effort}",
            "",
            "**è¯¦ç»†æ­¥éª¤**:"
        ])
        for detail in opt.details:
            lines.append(f"- {detail}")
        lines.append("")

    # 4. èŠ‚çœç»Ÿè®¡
    lines.extend([
        "## ğŸ“ˆ èŠ‚çœç»Ÿè®¡",
        "",
        f"**æ€»é¢„ä¼°èŠ‚çœ**: {savings['total_savings']:,} tokens",
        f"**ä¼˜åŒ–é¡¹æ•°**: {savings['optimization_count']}",
        "",
        "### æŒ‰ç±»å‹åˆ†ç±»",
        ""
    ])

    for type_name, stats in savings["by_type"].items():
        lines.append(f"- **{type_name}**: {stats['count']} é¡¹, {stats['savings']:,} tokens")

    lines.extend(["", "### æŒ‰ä¼˜å…ˆçº§åˆ†ç±»", ""])

    for priority, stats in savings["by_priority"].items():
        lines.append(f"- **{priority}**: {stats['count']} é¡¹, {stats['savings']:,} tokens")

    return "\n".join(lines)


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="AI Workflow ä¸Šä¸‹æ–‡åŠ è½½ä¼˜åŒ–å™¨")
    parser.add_argument(
        "--format",
        choices=["markdown", "json"],
        default="markdown",
        help="è¾“å‡ºæ ¼å¼ (é»˜è®¤: markdown)"
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="è¯¦ç»†è¾“å‡º"
    )

    args = parser.parse_args()

    # 1. åˆ†æ Prime åŠ è½½é€»è¾‘
    if args.verbose:
        print("ğŸ“– åˆ†æ wf_03_prime.md åŠ è½½é€»è¾‘...", file=sys.stderr)

    loading_analysis = analyze_prime_loading()

    # 2. åˆ†æ Docs ç´¢å¼•è¦†ç›–ç‡
    if args.verbose:
        print("ğŸ” åˆ†æ docs_index.json è¦†ç›–ç‡...", file=sys.stderr)

    total_docs, indexed_docs, missing_docs = analyze_docs_index()
    coverage = IndexCoverage(
        total_docs=total_docs,
        indexed_docs=indexed_docs,
        missing_docs=missing_docs,
        coverage_percentage=(indexed_docs / total_docs * 100) if total_docs > 0 else 0
    )

    if args.verbose:
        print(f"   æ€»æ–‡æ¡£: {total_docs}, å·²ç´¢å¼•: {indexed_docs}, è¦†ç›–ç‡: {coverage.coverage_percentage:.1f}%", file=sys.stderr)

    # 3. ç”Ÿæˆä¼˜åŒ–å»ºè®®
    if args.verbose:
        print("ğŸ’¡ ç”Ÿæˆä¼˜åŒ–å»ºè®®...", file=sys.stderr)

    optimizations = suggest_optimizations(loading_analysis, coverage)

    # 4. è®¡ç®—èŠ‚çœ
    savings = calculate_token_savings(optimizations)

    if args.verbose:
        print(f"   é¢„ä¼°æ€»èŠ‚çœ: {savings['total_savings']:,} tokens", file=sys.stderr)
        print("", file=sys.stderr)

    # 5. ç”ŸæˆæŠ¥å‘Š
    report = generate_report(loading_analysis, coverage, optimizations, savings, format=args.format)
    print(report)

    # é€€å‡ºç : 0 è¡¨ç¤ºä¸€åˆ‡æ­£å¸¸
    sys.exit(0)


if __name__ == "__main__":
    main()
